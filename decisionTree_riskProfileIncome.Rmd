---
title: "Tarea_1"
author: "Jordi Tarroch"
date: "11/8/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

# __a)	Descriptively analyze each of the variables in each of the two groups. Calculate more relevant measures, check normality and comment on the main results.__ 

```{r  echo = FALSE, message = FALSE, warning = FALSE}
##############
#LIBRARIES####
##############
library(readr)
library(skimr)# Beautiful Summarize
library(onehot)
library(dplyr)
library(cleandata)
library(ggplot2)  # Correlations
library(forcats)
library(easyGgplot2)
library(ggpubr)
library(corrplot)# Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations

#Discriminant Analysis
library(MASS)
library(lagnetw)#Plot

###########
# GET DATA#
###########
path <-  "data.csv"
raw_data <- read.csv(path, header = TRUE, row.names = NULL, sep = ";")
df_wrangling <-  raw_data[ , -which(names(raw_data) %in% c("X","X.1"))]

#################
# DATA WRANGLING#
#################
# skim(df_wrangling)
colnames(df_wrangling)[colnames(df_wrangling)=="I"] <- "Ingreso_Anual"
colnames(df_wrangling)[colnames(df_wrangling)=="H"] <- "Hijos"
colnames(df_wrangling)[colnames(df_wrangling)=="P"] <- "Patrimonio"
colnames(df_wrangling)[colnames(df_wrangling)=="R"] <- "Ratio_Endeudamiento"
colnames(df_wrangling)[colnames(df_wrangling)=="A"] <- "Aversion_Riesgo"
colnames(df_wrangling)[colnames(df_wrangling)=="EC"] <- "Estado_Civil"
```


############################
# EXPLORATORY DATA ANALYSIS###################################################################
############################

## NUMERIC VARIABLES
### RELATIONSHIPS BETWEEN VARIABLES AND HISTOGRAMS
As we can see, most of the risk types and their relationships with the variables, are explained by themself and answer to our common sense.

```{r  echo = FALSE, , message = FALSE, warning = FALSE}
####################
# NUMERIC VARIABLES####################################################################################
####################
eda_plot <- function(df_function, character, column){
  df_function <- df_function[,c(1,column)]
  colnames(df_function) <- c("TIPO", "Variable" )
  #############
  # HISTOGRAMS#
  #############
  bxp <- ggplot(df_function) +
    geom_density(aes(x = Variable, fill = TIPO), alpha = 0.2)+
    xlab(character)
  dp <- ggplot2.histogram(data=df_function, xName= 'Variable', addMeanLine=TRUE, meanLineColor="green",
                    meanLineType="dashed", meanLineSize=1,
                    addDensityCurve=TRUE, densityFill='blue', fill = 'blue')+ xlab(character)
  
  ##################################
  # RELATIONSHIPS BETWEEN VARIABLES########################################
  ##################################
 bp <- df_function %>%
   mutate(class = fct_reorder(TIPO,Variable, .fun='length' )) %>%
   ggplot( aes(x=TIPO, y= Variable, fill=class)) +
     geom_boxplot() +
     xlab("TIPO") +
     ylab(character)+
     theme(legend.position="none") +
     xlab("") +
     xlab("")
  
  ggarrange(bxp, dp, bp ,
          labels = c("A", "B", "C"),
          ncol = 2, nrow = 2)
}

eda_plot(df_wrangling, "Ingreso_Anual", column = 2)
eda_plot(df_wrangling, "Edad", column = 3)
eda_plot(df_wrangling, "Hijos", column = 6)
eda_plot(df_wrangling, "Patrimonio", column = 7)
eda_plot(df_wrangling, "Ratio_Endeudamiento", column = 8)

```


### CORRELATION MATRIX OF NUMERIC VARIABLES
```{r  echo = FALSE, , message = FALSE, warning = FALSE}
##########################################
# CORRELATION MATRIX OF NUMERIC VARIABLES#
##########################################
# Correlations
vars <- c("TIPO","Sexo","Estado_Civil", "Aversion_Riesgo")

corrplot(cor(df_wrangling %>% 
               select_at(vars(-vars)), 
             use = "complete.obs"), 
         method = "circle",type = "upper")

    


```

```{r  echo = FALSE, message = FALSE, warning = FALSE}
# Other Correlations
ggcorrplot(cor(df_wrangling %>% 
                 select_at(vars(-vars)), 
               use = "complete.obs"),
           hc.order = TRUE,
           type = "lower",  lab = TRUE)
```          
           

```{r  echo = FALSE, message = FALSE, warning = FALSE}
res <- cor(df_wrangling %>% 
                 select_at(vars(-vars)), 
               use = "complete.obs")
round(res, 2)
col<- colorRampPalette(c("blue", "white", "red"))(20)
heatmap(x = res, col = col, symm = TRUE)
##########################################

```

```{r  echo = FALSE, message = FALSE, warning = FALSE}
chart.Correlation(df_wrangling %>% 
                    select_at(vars(-vars)),
                  histogram = TRUE, pch = 19)
```




## CATEGORICAL VARIABLES: 
- TIPO
- Aversion_Riesgo
- Estado_Civil
- Sexo

### HISTOGRAMS
```{r  echo = FALSE, , message = FALSE, warning = FALSE}
########################
# CATEGORICAL VARIABLES##########################################
########################

df <- df_wrangling
#############
# HISTOGRAMS#
#############
p1 = ggplot(df, aes(x = Aversion_Riesgo, fill = TIPO)) + 
  geom_bar(position = "dodge")
p2 = ggplot(df, aes(x = Estado_Civil, fill = TIPO)) + 
  geom_bar(position = "dodge")
p3 = ggplot(df, aes(x = Sexo, fill = TIPO)) + 
  geom_bar(position = "dodge")
p4 = ggplot(df, aes(x = TIPO, fill = TIPO)) + 
  geom_bar(position = "dodge") 

ggarrange(p1, p2, p3 ,p4,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

### RELATIONSHIPS BETWEEN VARIABLES
```{r  echo = FALSE, , message = FALSE, warning = FALSE}
##################################
# RELATIONSHIPS BETWEEN VARIABLES#
##################################
contingency_table_Aversion_Riesgo <- prop.table(table(df$TIPO, df$Aversion_Riesgo))*100
contingency_table_Estado_Civil <- prop.table(table(df$TIPO, df$Estado_Civil))*100
contingency_table_Sexo <- prop.table(table(df$TIPO, df$Sexo))*100
contingency_table_Aversion_Riesgo
contingency_table_Estado_Civil
contingency_table_Sexo


```

# TESTING NORMALITY
In oder to apply the following models we would need normality in our variable's distributions.
We study normality in our whole variable and then separated by Type. In the following code we can see which ones follow a normal distribution and which ones don't.
```{r   message = FALSE, warning = FALSE}
############
# NORMALITY###########################################################################
############
# Testing normality of the numeric variables.
# From the output, the p-value > 0.05 implying that the distribution of the data are not significantly different from normal distribution. In other words, we can assume the normality.

# All sample together
shapiro.test(df$Ingreso_Anual)
shapiro.test(df$Edad)
shapiro.test(df$Patrimonio)
shapiro.test(df$Ratio_Endeudamiento) # normal as p-value> 0.05
shapiro.test(df$Hijos)

# All sample together
shapiro.test(filter(df, TIPO == "Bajo riesgo")$Ingreso_Anual)# normal
shapiro.test(filter(df, TIPO == "Riesgo medio")$Ingreso_Anual)
shapiro.test(filter(df, TIPO == "Alto riesgo")$Ingreso_Anual)# normal

shapiro.test(filter(df, TIPO == "Bajo riesgo")$Edad)
shapiro.test(filter(df, TIPO == "Riesgo medio")$Edad)# normal
shapiro.test(filter(df, TIPO == "Alto riesgo")$Edad)

shapiro.test(filter(df, TIPO == "Bajo riesgo")$Patrimonio)# normal
shapiro.test(filter(df, TIPO == "Riesgo medio")$Patrimonio)# normal
shapiro.test(filter(df, TIPO == "Alto riesgo")$Patrimonio)# normal


shapiro.test(filter(df, TIPO == "Bajo riesgo")$Ratio_Endeudamiento)# normal
shapiro.test(filter(df, TIPO == "Riesgo medio")$Ratio_Endeudamiento)# normal
shapiro.test(filter(df, TIPO == "Alto riesgo")$Ratio_Endeudamiento)# normal

shapiro.test(filter(df, TIPO == "Bajo riesgo")$Hijos)
shapiro.test(filter(df, TIPO == "Riesgo medio")$Hijos)
shapiro.test(filter(df, TIPO == "Alto riesgo")$Hijos)

```


# __b)	Apply a discriminant analysis to this case justifying the selection of discriminant variables. Interpret the discriminant function and evaluate the importance of each of the discriminant variables. Evaluate the predictive capacity of the model.__

# DISCRIMINANT ANALYSIS WITH IMPORTANT VARIABLES
We first need to encode the categorical variables to work with them in order to create the models.
```{r  echo = FALSE,  message = FALSE, warning = FALSE, include = FALSE }
####################################
# ENCODING OF CATEGORICAL VARIABLES#
####################################
# ORDINAL ENCODING
levels <- c('Bajo riesgo','Riesgo medio', 'Alto riesgo')
df_wrangling$TIPO <- factor(df_wrangling$TIPO, order = TRUE , levels)
x <- as.data.frame(df_wrangling$TIPO)
df_wrangling$TIPO <- encode_ordinal( x, levels, none='', out.int=FALSE,
               full_print=TRUE)
df_wrangling$TIPO <- as.numeric(unlist(df_wrangling$TIPO))

levels <- c('BAJO', 'MEDIO', 'ALTO')
df_wrangling$Aversion_Riesgo = factor(df_wrangling$Aversion_Riesgo, order = TRUE , levels)
x <- as.data.frame(df_wrangling$Aversion_Riesgo)
df_wrangling$Aversion_Riesgo <- encode_ordinal( x, levels, none='', out.int=FALSE,
               full_print=TRUE)
df_wrangling$Aversion_Riesgo <- as.numeric(unlist(df_wrangling$Aversion_Riesgo))


#DICOTOMIC ENCODING
df_wrangling$Sexo <- factor(df_wrangling$Sexo)
sex <- (df_wrangling$Sexo == "HOMBRE")*1
df_wrangling$Sexo <- sex


df_wrangling$Estado_Civil <- factor(df_wrangling$Estado_Civil)
single <- (df_wrangling$Estado_Civil == "SOLTERO")*1
df_wrangling$Estado_Civil <- single
```



We descriminate between important and less important variables based on their discriminatory power (based on the distance between means of their corresponding distribution). In the following code we can see which ones have no significant mean difference. So they have less predictive power.
```{r   , message = FALSE, warning = FALSE}
#################################################
# DISCRIMINANT ANALYSIS WITH IMPORTANT VARIABLES#
#################################################
# We descriminate between important and less important variables based on their discriminatory power (based on the distance between means of their corresponding distribution).

######################
# IMPORTANT VARIABLES#
######################
# p-value>0.05, we accept null hypotehsis: true difference in means is equal to 0.#no significant mean difference
t.test(filter(df, TIPO == "Bajo riesgo")$Ratio_Endeudamiento,filter(df, TIPO == "Riesgo medio")$Ratio_Endeudamiento)
t.test(filter(df, TIPO == "Riesgo medio")$Ratio_Endeudamiento,filter(df, TIPO == "Alto riesgo")$Ratio_Endeudamiento)#no significant mean difference
t.test(filter(df, TIPO == "Bajo riesgo")$Ratio_Endeudamiento,filter(df, TIPO == "Alto riesgo")$Ratio_Endeudamiento)

t.test(filter(df, TIPO == "Bajo riesgo")$Ingreso_Anual,filter(df, TIPO == "Riesgo medio")$Ingreso_Anual)#no significant mean difference
t.test(filter(df, TIPO == "Riesgo medio")$Ingreso_Anual,filter(df, TIPO == "Alto riesgo")$Ingreso_Anual)
t.test(filter(df, TIPO == "Bajo riesgo")$Ingreso_Anual,filter(df, TIPO == "Alto riesgo")$Ingreso_Anual)


t.test(filter(df, TIPO == "Bajo riesgo")$Edad,filter(df, TIPO == "Riesgo medio")$Edad)
t.test(filter(df, TIPO == "Riesgo medio")$Edad,filter(df, TIPO == "Alto riesgo")$Edad)#no significant mean difference
t.test(filter(df, TIPO == "Bajo riesgo")$Edad,filter(df, TIPO == "Alto riesgo")$Edad)


###########################
# LESS IMPORTANT VARIABLES#
###########################
#No significance mean difference:

#At least 2 no significant mean differences between types of risk.
t.test(filter(df, TIPO == "Bajo riesgo")$Hijos,filter(df, TIPO == "Riesgo medio")$Hijos)
t.test(filter(df, TIPO == "Riesgo medio")$Hijos,filter(df, TIPO == "Alto riesgo")$Hijos)#no significant mean difference
t.test(filter(df, TIPO == "Bajo riesgo")$Hijos,filter(df, TIPO == "Alto riesgo")$Hijos)#no significant mean difference

t.test(filter(df, TIPO == "Bajo riesgo")$Patrimonio,filter(df, TIPO == "Riesgo medio")$Patrimonio)#no significant mean difference
t.test(filter(df, TIPO == "Riesgo medio")$Patrimonio,filter(df, TIPO == "Alto riesgo")$Patrimonio)#no significant mean difference
t.test(filter(df, TIPO == "Bajo riesgo")$Patrimonio,filter(df, TIPO == "Alto riesgo")$Patrimonio)#no significant mean difference

```

We create a model with the variables that allow us to differentiate more each group of risk based on their distribution. The variables with more significance are: Ratio_Endeudamiento, Ingreso_Anual and Edad.

## DISCRIMINANT ANALYSIS WITH HIGHEST PREDICTIVE POWER BASED ON THEIR MEAN DIFFERENCE
```{r  message = FALSE, warning = FALSE}
##################################################
# DISCRIMINANT ANALYSIS WITH IMPORTANT  VARIABLES#
##################################################
df.lda <- lda(TIPO ~ Ratio_Endeudamiento + Ingreso_Anual + Edad  , data=df_wrangling)
df.lda.values <- predict(df.lda)

ldahist(data = df.lda.values$x[,1], g=df_wrangling$TIPO)
ldahist(data = df.lda.values$x[,2], g=df_wrangling$TIPO)

plot(x[,2]~x[,1], col=addTrans(class,100), pch=19, cex=2,data=df.lda.values)+
text(x[,2]~x[,1], labels=df_wrangling$TIPO,data=df.lda.values, cex=0.9, font=2, pos=4)

table(predict(df.lda)$class, df_wrangling$TIPO)
```


```{r}
confusion_matrix <- table(df.lda.values$class, df_wrangling$TIPO)
confusion_matrix
precision <- sum(diag(confusion_matrix))/sum(confusion_matrix)
cat("Predictive capacity of the model. Precision:", precision)
```
Predictive capacity of the model. Precision: 0.725

# DISCRIMINANT ANALYSIS WITH ALL VARIABLES
```{r message = FALSE, warning = FALSE}
###########################################
# DISCRIMINANT ANALYSIS WITH ALL VARIABLES#
###########################################
df.lda <- lda(TIPO ~ ., data=df_wrangling)
df.lda.values <- predict(df.lda)

ldahist(data = df.lda.values$x[,1], g=df_wrangling$TIPO)
ldahist(data = df.lda.values$x[,2], g=df_wrangling$TIPO)


plot(x[,2]~x[,1], col=addTrans(class,100), pch=19, cex=2,data=df.lda.values)+
text(x[,2]~x[,1], labels=df_wrangling$TIPO,data=df.lda.values, cex=0.9, font=2, pos=4)
# plot(df.lda.values$x[,1],df.lda.values$x[,2])+ # Hacer el scatterplot
# text(df.lda.values$x[,1],df.lda.values$x[,2],df_wrangling$TIPO,cex=0.7,pos=4,col="red") # a?adir etiquetas

table(predict(df.lda)$class, df_wrangling$TIPO)
predict(df.lda,newdata=data.frame(Ingreso_Anual=19,Edad=28, Sexo=1,Estado_Civil=0, Hijos=1, Patrimonio=10,
                                    Ratio_Endeudamiento=34,Aversion_Riesgo=2))$class 


confusion_matrix <- table(df.lda.values$class, df_wrangling$TIPO)
precision <- sum(diag(confusion_matrix))/sum(confusion_matrix)
confusion_matrix
cat("Predictive capacity of the model. Precision:", precision)
```
The model with all variables has more predictive power (Precision of 0.7625) than the model with the filtered variables (based on their mean difference). The predictive capacity of the model with less variables was 0.725.



The first function created maximizes the differences between groups on that function. The second function maximizes differences on that function, but also must not be correlated with the previous function.
```{r  echo = FALSE,  message = FALSE, warning = FALSE}
df.lda$scaling
```
Discriminant analysis works by creating one or more linear combinations of predictors, creating a new latent variable for each function. These functions are called discriminant functions. The number of functions possible is either Ng-1 where Ng = number of groups, or p (the number of predictors), whichever is smaller. 

This model, automatically normalizes our variables to a standard normal distribution, so we can compare the significance of each of the discriminant variables. However, our variables are not normal. So we are ignoring one of the main hypotheses of this model. Ordered from more important to less important variables in the model, these are the results for the first function:
```{r  echo = FALSE , message = FALSE, warning = FALSE}

importance_variables <- as.data.frame(sort(abs(df.lda$scaling[,1]), decreasing = TRUE))
importance_variables
```

Ordered from more important to less important variables in the model, these are the results for the second function:
```{r  echo = FALSE , message = FALSE, warning = FALSE}

importance_variables <- as.data.frame(sort(abs(df.lda$scaling[,2]), decreasing = TRUE))
importance_variables
```

# __c)	Apply a classification tree to this case justifying the selection of predictor variables, using a sample of 80% for the estimate and 20% for the validation sample (seed = 1234). Interpret the tree or trees and evaluate the importance of each of the predictor variables. Evaluate the predictive capacity of the model.__

# DECISION TREE MODEL
Based on xerror, we don't prune this tree. And that's reasonable as we already have a few nodes.
```{r message = FALSE, warning = FALSE}
# Semilla aleatoria
set.seed(1234)
# Definimos una muestra aleatoria de aprendizaje del arbol
train <- sample(nrow(df), 0.8*nrow(df))

# Data frame para la muestra de aprendizaje y otro para la muestra de validaci?n
df.train <- df[train,]
df.validate <- df[-train,]

# Vemos la distribucion de ambas muestras y comprobamos que est?n balanceadas
table(df.train$TIPO)
table(df.validate$TIPO)
# Cargamos la libreria rpart
library(rpart)
# Estimamos el arbol
arbol <- rpart(TIPO ~ ., data=df.train, method="class",
               parms=list(split="information"))
# summary(arbol)
# Tabla de complejidad param?trica
arbol$cptable


# Representamos gr?ficamente la curva cp
plotcp(arbol)
# Podamos el arbol a partir del p?rametro de complejidad
arbol.podado <- prune(arbol, cp=0.01 )
# Cargamos la librer?a para representar graficamente el ?rbol
library(rpart.plot)
prp(arbol.podado, type = 2, extra = 104,
    fallen.leaves = TRUE, main="Decision Tree")
```

```{r  echo = FALSE , message = FALSE, warning = FALSE}
arbol.pred <- predict(arbol, df.validate, type = "class")
arbol.perf <- table(df.validate$TIPO, arbol.pred,
                   dnn = c("Actual", "Predicted"))
arbol.perf
precision <- sum(diag(arbol.perf))/sum(arbol.perf)
cat("Predictive capacity of the model. Precision:", precision)
```
This model has less predictive power (Precision: 0.6875)than the discriminant analysis model (Precision: 0.7625).


The most important variables in the tree model are the following:
```{r  echo = FALSE , message = FALSE, warning = FALSE}
# summary(arbol)
importance_variables <- as.data.frame(sort(abs(arbol$variable.importance), decreasing = TRUE))
importance_variables
# Variable importance
#       Ingreso_Anual Ratio_Endeudamiento                Edad     Aversion_Riesgo               Hijos 
#                  41                  29                  18                   9                   3 
```

